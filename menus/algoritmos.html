<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Portafolio de Juan Lupi</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="../assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="../assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">

	<!-- Page Wrapper -->
	<div id="page-wrapper">

		<!-- Header -->
		<header id="header">
			<h1><a href="../index.html">Portafolio de Juan Lupi</a></h1>
			<nav>
				<a href="#menu">Menu</a>
			</nav>
		</header>

		<!-- Menu -->
		<nav id="menu">
			<div class="inner">
				<h2>Menu</h2>
				<ul class="links">
					<li><a href="../index.html">Home</a></li>
					<li><a href="marcoTeorico.html">Marco Teórico</a></li>
					<li><a href="casosDeEstudio.html">Casos de estudio</a></li>
					<li><a href="algoritmos.html">Algoritmos</a></li>
				</ul>
				<a href="#" class="close">Close</a>
			</div>
		</nav>

		<!-- Wrapper -->
		<section id="wrapper">
			<header>
				<div class="inner">
					<h2>Algoritmos</h2>
					<p>En este espacio, profundizaremos en los análisis de los algoritmos que hemos explorado
						previamente.
						A medida que recorramos estos análisis, desglosaremos el funcionamiento interno de cada
						algoritmo,
						evaluaremos su desempeño en una variedad de escenarios y discutiremos las mejores prácticas para
						su implementación.</p>
				</div>
			</header>

			<!-- Content -->
			<div class="wrapper">
				<div class="inner">
					<h3 class="major">Algoritmos Lineales</h3>
					<section class="features">
						<article>
							<a href="../menus/algoritmos/regresionLineal.html" class="image"><img
									src="../images/regresinlineal.ppm" alt="" /></a>
							<h3 class="major">Regresión Lineal</h3>
							<a href="../menus/algoritmos/regresionLineal.html" class="special">Ver más</a>
						</article>
						<article>
							<a href="../menus/algoritmos/descensogradiente.html" class="image"><img src="../images/descensogradiente.png" alt="" /></a>
							<h3 class="major">Descenso de Gradiente</h3>
							
							<a href="../menus/algoritmos/descensogradiente.html" class="special">Ver más</a>
						</article>
						<article>
							<a href="../menus/algoritmos/analisisDisLin.html" class="image"><img src="../images/lda.png" alt="" /></a>
							<h3 class="major">Análisis Discriminante Lineal</h3>
							
							<a href="../menus/algoritmos/analisisDisLin.html" class="special">Ver más</a>
						</article>
						<article>
							<a href="../menus/algoritmos/regresionLogistica.html" class="image"><img src="../images/regresionLogistica.jpg" alt="" /></a>
							<h3 class="major">Regresión Logística</h3>
							
							<a href="../menus/algoritmos/regresionLogistica.html" class="special">Ver más</a>
						</article>
					</section>

					<h3 class="major">Algoritmos No Lineales</h3>
					<section class="features">
						<article>
							<a href="../menus/algoritmos/Knn.html" class="image"><img src="../images/knn.png" alt="" /></a>
							<h3 class="major">k-Nearest Neighbors</h3>
							
							<a href="../menus/algoritmos/Knn.html" class="special">Ver más</a>
						</article>
						<article>
							<a href="../menus/algoritmos/naiveBayes.html" class="image"><img src="../images/naivebayes.png" alt="" /></a>
							<h3 class="major">Naive Bayes</h3>
							
							<a href="../menus/algoritmos/naiveBayes.html" class="special">Ver más</a>
						</article>
					</section>

					<h3 class="major">Feature Selection</h3>
					<p>
						La selección de características, o feature selection, es un proceso esencial en la preparación y
						el análisis de datos. Consiste en identificar y elegir un subconjunto de atributos
						(características) relevantes de un conjunto de datos, mientras se descartan los atributos menos
						informativos o redundantes. El objetivo principal de la selección de características es mejorar
						la eficiencia del modelo de aprendizaje automático y la calidad de las predicciones. Algunos de
						los beneficios clave de la selección de características incluyen:
						<br>
						Reducción de la dimensionalidad: Al eliminar características irrelevantes o redundantes, se
						reduce la cantidad de datos con los que se trabaja, lo que simplifica el análisis y el modelado.
						<br>
						Mejora del rendimiento: Al enfocarse en las características más importantes, se pueden construir
						modelos más simples y eficaces, lo que a menudo conduce a un mejor rendimiento predictivo.
						<br>
						Ahorro de tiempo y recursos: Al reducir la cantidad de características, se acelera el proceso de
						entrenamiento y evaluación de modelos, lo que ahorra tiempo y recursos computacionales.
						<br>
						Mejor comprensión: La selección de características puede ayudar a comprender mejor las
						relaciones entre las características y la variable objetivo, lo que aporta conocimiento
						adicional sobre el problema.
						<br>
						Existen diversas técnicas para llevar a cabo la selección de características, que van desde
						enfoques estadísticos hasta algoritmos de aprendizaje automático. Algunos ejemplos incluyen el
						análisis de correlación, la eliminación recursiva de características, la importancia de
						características según modelos, y más.
						<br>
						En resumen, la selección de características es una fase crucial en el proceso de análisis de
						datos y modelado de aprendizaje automático, que permite identificar y utilizar las
						características más relevantes para mejorar la calidad y la eficiencia de los modelos
						predictivos.
					</p>
					<h4>Principales técincas de Feature Selection</h4>
					<p>
						<strong>Forward Selection:</strong>

						Forward Selection es una técnica utilizada en el proceso de selección de características para
						construir un modelo de aprendizaje automático. Su enfoque es incremental, comenzando con un
						conjunto vacío de características y agregando una a la vez. El proceso típico de Forward
						Selection se realiza de la siguiente manera:
						<br>
						Comienza con un conjunto vacío de características.
						<br>
						Entrena un modelo utilizando cada característica individual y evalúa su rendimiento utilizando
						una métrica adecuada (por ejemplo, precisión, RMSE, etc.).
						<br>
						Selecciona la característica que produzca el mejor rendimiento y la agrega al conjunto.
						<br>
						Repite el proceso, agregando una característica adicional en cada iteración, y evalúa el
						rendimiento del modelo en función de las características seleccionadas hasta ese momento.
						<br>
						Continúa este proceso hasta que se alcance un criterio de detención predefinido, como un número
						máximo de características o una métrica de rendimiento que deje de mejorar.
						<br>
						<br>
						<strong>Backward Elimination:</strong>
						<br>
						Backward Elimination es otra técnica de selección de características que trabaja de manera
						opuesta a Forward Selection. Comienza con un conjunto completo de características y, en cada
						iteración, elimina la característica que tiene el menor impacto en el rendimiento del modelo. El
						proceso de Backward Elimination se realiza de la siguiente manera:
						<br>
						Comienza con un conjunto completo de características.
						<br>
						Entrena un modelo utilizando todas las características y evalúa su rendimiento.
						<br>
						Identifica la característica cuya eliminación tiene el menor impacto en el rendimiento del
						modelo.
						<br>
						Elimina la característica identificada en el paso anterior.
						<br>
						Repite el proceso, eliminando una característica en cada iteración, y evalúa el rendimiento del
						modelo en función de las características restantes.
						<br>
						Continúa este proceso hasta que se alcance un criterio de detención predefinido, como un número
						mínimo de características o una métrica de rendimiento que deje de mejorar.
						<br>
						Ambas técnicas, Forward Selection y Backward Elimination, buscan encontrar el conjunto óptimo de
						características que maximice el rendimiento del modelo mientras se mantiene la simplicidad y la
						eficiencia computacional. La elección entre estas dos técnicas depende de la naturaleza del
						problema y la cantidad de características disponibles. Además, existen métodos más avanzados que
						combinan elementos de ambas técnicas para encontrar un conjunto de características óptimo.
					</p>
				</div>
			</div>

		</section>
	</div>

	<!-- Scripts -->
	<script src="../assets/js/jquery.min.js"></script>
	<script src="../assets/js/jquery.scrollex.min.js"></script>
	<script src="../assets/js/browser.min.js"></script>
	<script src="../assets/js/breakpoints.min.js"></script>
	<script src="../assets/js/util.js"></script>
	<script src="../assets/js/main.js"></script>

</body>

</html>